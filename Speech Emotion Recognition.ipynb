{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b46849",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be2626",
   "metadata": {},
   "source": [
    "Speech Emotion Recognition (SER) is the endeavor to identify human emotions and affective states conveyed through speech. It leverages the observation that emotions are often conveyed through variations in tone and pitch. Interestingly, this ability to interpret emotions from speech is not exclusive to humans, as animals like dogs and horses also exhibit this capability in understanding human emotions.\n",
    "The complexity of SER arises due to the subjective nature of emotions and the difficulty in annotating audio data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50b44f",
   "metadata": {},
   "source": [
    "Object: To build a model to recognize emotion from speech using the librosa and sklearn libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18664dbf",
   "metadata": {},
   "source": [
    "In this project, we will use the libraries librosa, soundfile, and sklearn (among others) to build a model using an MLPClassifier. This will be able to recognize emotion from sound files. We will load the data, extract features from it, then split the dataset into training and testing sets. Then, we’ll initialize an MLPClassifier and train the model. Finally, we’ll calculate the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617ad5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in c:\\users\\asus laptop\\anaconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\asus laptop\\anaconda3\\lib\\site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus laptop\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c69ac",
   "metadata": {},
   "source": [
    "librosa is a Python library for analyzing audio and music. It has a flatter package layout, standardizes interfaces and names, backwards compatibility, modular functions, and readable code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff659125",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504cab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudio\n",
      "  Downloading PyAudio-0.2.14-cp311-cp311-win_amd64.whl (164 kB)\n",
      "                                              0.0/164.1 kB ? eta -:--:--\n",
      "     --                                       10.2/164.1 kB ? eta -:--:--\n",
      "     --                                       10.2/164.1 kB ? eta -:--:--\n",
      "     --                                       10.2/164.1 kB ? eta -:--:--\n",
      "     --                                       10.2/164.1 kB ? eta -:--:--\n",
      "     --                                       10.2/164.1 kB ? eta -:--:--\n",
      "     --                                       10.2/164.1 kB ? eta -:--:--\n",
      "     --                                       10.2/164.1 kB ? eta -:--:--\n",
      "     -------                                30.7/164.1 kB 69.0 kB/s eta 0:00:02\n",
      "     -------                                30.7/164.1 kB 69.0 kB/s eta 0:00:02\n",
      "     -------                                30.7/164.1 kB 69.0 kB/s eta 0:00:02\n",
      "     -------                                30.7/164.1 kB 69.0 kB/s eta 0:00:02\n",
      "     -------                                30.7/164.1 kB 69.0 kB/s eta 0:00:02\n",
      "     -------                                30.7/164.1 kB 69.0 kB/s eta 0:00:02\n",
      "     ---------                              41.0/164.1 kB 56.2 kB/s eta 0:00:03\n",
      "     ----------------                       71.7/164.1 kB 98.3 kB/s eta 0:00:01\n",
      "     ----------------                       71.7/164.1 kB 98.3 kB/s eta 0:00:01\n",
      "     ----------------                       71.7/164.1 kB 98.3 kB/s eta 0:00:01\n",
      "     ----------------                       71.7/164.1 kB 98.3 kB/s eta 0:00:01\n",
      "     ---------------------                  92.2/164.1 kB 98.9 kB/s eta 0:00:01\n",
      "     ------------------------             112.6/164.1 kB 112.9 kB/s eta 0:00:01\n",
      "     ------------------------             112.6/164.1 kB 112.9 kB/s eta 0:00:01\n",
      "     ------------------------             112.6/164.1 kB 112.9 kB/s eta 0:00:01\n",
      "     ------------------------             112.6/164.1 kB 112.9 kB/s eta 0:00:01\n",
      "     --------------------------           122.9/164.1 kB 103.0 kB/s eta 0:00:01\n",
      "     --------------------------           122.9/164.1 kB 103.0 kB/s eta 0:00:01\n",
      "     -------------------------------      143.4/164.1 kB 112.1 kB/s eta 0:00:01\n",
      "     -------------------------------      143.4/164.1 kB 112.1 kB/s eta 0:00:01\n",
      "     -------------------------------      143.4/164.1 kB 112.1 kB/s eta 0:00:01\n",
      "     -------------------------------      143.4/164.1 kB 112.1 kB/s eta 0:00:01\n",
      "     -----------------------------------  163.8/164.1 kB 108.0 kB/s eta 0:00:01\n",
      "     ------------------------------------ 164.1/164.1 kB 105.9 kB/s eta 0:00:00\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d03ed",
   "metadata": {},
   "source": [
    "Steps for speech emotion recognition python projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b0d84",
   "metadata": {},
   "source": [
    "1. Make the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71fa2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854ca5f",
   "metadata": {},
   "source": [
    "2. Define a function extract_feature to extract the mfcc, chroma, and mel features from a sound file. \n",
    "This function takes 4 parameters- the file name and three Boolean parameters for the three features:\n",
    "   - mfcc: Mel Frequency Cepstral Coefficient, represents the short-term power spectrum of a sound\n",
    "   - chroma: Pertains to the 12 different pitch classes\n",
    "   - mel: Mel Spectrogram Frequency\n",
    "   \n",
    " Open the sound file with soundfile.SoundFile using with-as so it’s automatically closed once we’re done. Read from it and call it X. Also, get the sample rate. If chroma is True, get the Short-Time Fourier Transform of X.\n",
    "\n",
    "Let result be an empty numpy array. Now, for each feature of the three, if it exists, make a call to the corresponding function from librosa.feature (eg- librosa.feature.mfcc for mfcc), and get the mean value. Call the function hstack() from numpy with result and the feature value, and store this in result. hstack() stacks arrays in sequence horizontally (in a columnar fashion). Then, return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d54133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfb0d8",
   "metadata": {},
   "source": [
    "3. Now, let’s define a dictionary to hold numbers and the emotions available in the dataset, and a list to hold those we want- calm, happy, fearful, disgust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f292e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Emotions in the RAVDESS dataset\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "\n",
    "#DataFlair - Emotions to observe\n",
    "observed_emotions=['calm', 'happy', 'fearful', 'disgust']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af103b",
   "metadata": {},
   "source": [
    "4. Now, let’s load the data with a function load_data() – this takes in the relative size of the test set as parameter. x and y are empty lists; we’ll use the glob() function from the glob module to get all the pathnames for the sound files in our dataset. \n",
    "Using our emotions dictionary, this number is turned into an emotion, and our function checks whether this emotion is in our list of observed_emotions; if not, it continues to the next file. It makes a call to extract_feature and stores what is returned in ‘feature’. Then, it appends the feature to x and the emotion to y. So, the list x holds the features and y holds the emotions. We call the function train_test_split with these, the test size, and a random state value, and return that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f00c1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data and extract features for each sound file\n",
    "def load_data(test_size):\n",
    "    x,y=[],[]\n",
    "    for file in glob.glob(\"H:\\Data Projects\\data scientist\\speech-emotion-recognition-ravdess-data\\Actor_*\\*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89696b23",
   "metadata": {},
   "source": [
    "5.Time to split the dataset into training and testing sets! Let’s keep the test set 25% of everything and use the load_data function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63eec5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Split the dataset\n",
    "x_train,x_test,y_train,y_test=load_data(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80cbaa",
   "metadata": {},
   "source": [
    "6. Observe the shape of the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76b70c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 154)\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the training and testing datasets\n",
    "print((x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c697d19",
   "metadata": {},
   "source": [
    "7. And get the number of features extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af685e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: 180\n"
     ]
    }
   ],
   "source": [
    "#Get the number of features extracted\n",
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7f466",
   "metadata": {},
   "source": [
    "8. Now, let’s initialize an MLPClassifier. This is a Multi-layer Perceptron Classifier; it optimizes the log-loss function using LBFGS or stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13f6a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the Multi Layer Perceptron Classifier\n",
    "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08cbff2",
   "metadata": {},
   "source": [
    "9. Fit/train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8cd3064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=500)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate='adaptive', max_iter=500)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFlair - Train the model\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2dea49",
   "metadata": {},
   "source": [
    "10.Let’s predict the values for the test set. This gives us y_pred (the predicted emotions for the features in the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eba2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Predict for the test set\n",
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a8ce2",
   "metadata": {},
   "source": [
    "11.To calculate the accuracy of our model, we’ll call up the accuracy_score() function we imported from sklearn. Finally, we’ll round the accuracy to 2 decimal places and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d7a98a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.03%\n"
     ]
    }
   ],
   "source": [
    "#DataFlair - Calculate the accuracy of our model\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "#DataFlair - Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3914755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
